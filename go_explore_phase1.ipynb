{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Go-Explore: Phase 1 Implementation\n",
        "## A New Approach for Hard-Exploration Problems\n",
        "\n",
        "**Paper Reference:**  \n",
        "Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019).  \n",
        "*Go-Explore: A New Approach for Hard-Exploration Problems*  \n",
        "arXiv preprint arXiv:1901.10995\n",
        "\n",
        "**Paper Link:** https://huggingface.co/papers/1901.10995\n",
        "\n",
        "**Implementation:** Phase 1 (\"Explore Until Solved\") only  \n",
        "**Environment:** FrozenLake-v1 (deterministic, is_slippery=False)  \n",
        "**Target:** Graduate-level Reinforcement Learning midterm project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction and Summary\n",
        "\n",
        "### What is Go-Explore?\n",
        "\n",
        "Go-Explore is a novel exploration algorithm designed to solve \"hard-exploration\" problems in reinforcement learning. Traditional RL algorithms often struggle in sparse-reward environments due to two key failure modes:\n",
        "\n",
        "1. **Detachment**: The agent forgets how to return to promising states after exploring further.\n",
        "2. **Derailment**: Small stochastic perturbations cause the agent to deviate from promising trajectories, making it difficult to reproduce successful behaviors.\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "Go-Explore addresses these issues through a simple yet powerful principle: **\"Remember promising states and systematically return to them to explore further.\"**\n",
        "\n",
        "The algorithm maintains an **archive** of visited states (represented as abstract \"cells\") along with the trajectories needed to reach them. It then:\n",
        "1. Selects a promising cell from the archive\n",
        "2. **Returns** to that cell deterministically (solving detachment)\n",
        "3. **Explores** from that cell with random actions\n",
        "4. Adds any newly discovered cells to the archive\n",
        "\n",
        "### Two-Phase Approach\n",
        "\n",
        "- **Phase 1 (\"Explore Until Solved\")**: Use the archive-based exploration to find a solution in a deterministic environment\n",
        "- **Phase 2 (\"Robustification\")**: Train a robust policy via imitation learning (not implemented here)\n",
        "\n",
        "### This Notebook\n",
        "\n",
        "This notebook implements **Phase 1 only**, demonstrating the core exploration mechanism of Go-Explore in the simple FrozenLake environment. We show how the \"return-then-explore\" strategy systematically discovers all reachable states, solving the exploration problem that stymies standard random exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install gymnasium numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Algorithm Explanation: Phase 1 (\"Explore Until Solved\")\n",
        "\n",
        "### Key Components\n",
        "\n",
        "#### 3.1 State Abstraction: Cells\n",
        "\n",
        "Go-Explore groups similar states into abstract **cells**. In our FrozenLake implementation, each grid position naturally corresponds to a unique cell (the agent's position). This abstraction:\n",
        "- Reduces memory requirements\n",
        "- Makes the archive more manageable\n",
        "- Focuses exploration on meaningfully different states\n",
        "\n",
        "#### 3.2 The Archive\n",
        "\n",
        "The archive is the heart of Go-Explore. It stores:\n",
        "- **Cell representation**: An abstract representation of the state\n",
        "- **Trajectory**: The sequence of actions needed to reach that cell from the start\n",
        "- **Reward**: The cumulative reward obtained along that trajectory\n",
        "\n",
        "The archive grows as we discover new cells during exploration.\n",
        "\n",
        "#### 3.3 Return-Then-Explore\n",
        "\n",
        "The algorithm follows a simple loop:\n",
        "\n",
        "1. **Select**: Choose a cell from the archive (randomly or by heuristic)\n",
        "2. **Return**: Deterministically execute the stored trajectory to reach that cell\n",
        "3. **Explore**: Take K random exploratory actions from that cell\n",
        "4. **Update**: Add any newly discovered cells to the archive\n",
        "\n",
        "This approach solves both failure modes:\n",
        "- **Detachment**: We never \"forget\" how to return to promising statesâ€”the trajectory is stored\n",
        "- **Derailment**: We return deterministically (no stochasticity), ensuring we reliably reach the chosen cell\n",
        "\n",
        "#### 3.4 Why This Works\n",
        "\n",
        "Traditional random exploration struggles because:\n",
        "- Randomly revisiting a specific state is exponentially unlikely in large state spaces\n",
        "- Once we've moved away from a promising state, we rarely return to it\n",
        "\n",
        "Go-Explore systematically revisits all discovered states, ensuring comprehensive exploration without relying on rare random events.\n",
        "\n",
        "### Pseudo-code\n",
        "\n",
        "```\n",
        "Initialize archive with starting state\n",
        "while not solved:\n",
        "    cell = select_cell_from_archive()\n",
        "    return_to_cell(cell)  # Execute stored trajectory\n",
        "    for k in range(K):\n",
        "        action = random_action()\n",
        "        state, reward = step(action)\n",
        "        cell_new = get_cell(state)\n",
        "        if cell_new not in archive or reward > archive[cell_new].reward:\n",
        "            add_to_archive(cell_new, trajectory, reward)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implementation\n",
        "\n",
        "### 4.1 Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FrozenLake environment with deterministic dynamics (is_slippery=False)\n",
        "# This ensures our \"return\" phase works perfectly - critical for Phase 1\n",
        "# Using 8x8 map for a more challenging exploration problem (64 cells instead of 16)\n",
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode=None)\n",
        "\n",
        "print(f\"Environment: {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Actions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
        "print(\"\\nFrozenLake Map:\")\n",
        "print(\"S = Start, F = Frozen (safe), H = Hole (terminal), G = Goal (terminal)\")\n",
        "print(env.unwrapped.desc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Core Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cell(state):\n",
        "    \"\"\"\n",
        "    State abstraction function: converts raw state to a cell representation.\n",
        "    \n",
        "    In FrozenLake, the state is already an integer representing grid position,\n",
        "    so we can use it directly as our cell. In more complex environments,\n",
        "    this might involve downsampling images or discretizing continuous states.\n",
        "    \n",
        "    Args:\n",
        "        state: The raw environment state\n",
        "        \n",
        "    Returns:\n",
        "        cell: Abstract cell representation\n",
        "    \"\"\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def rollout_to_cell(env, trajectory):\n",
        "    \"\"\"\n",
        "    Deterministically return to a cell by executing the stored trajectory.\n",
        "    \n",
        "    This is the \"return\" phase that solves the detachment problem.\n",
        "    Since our environment is deterministic (is_slippery=False), replaying\n",
        "    the same actions always reaches the same state.\n",
        "    \n",
        "    Args:\n",
        "        env: The environment\n",
        "        trajectory: List of actions to execute\n",
        "        \n",
        "    Returns:\n",
        "        state: The final state after executing the trajectory\n",
        "        total_reward: Cumulative reward obtained\n",
        "        terminated: Whether the episode terminated\n",
        "    \"\"\"\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    terminated = False\n",
        "    \n",
        "    for action in trajectory:\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    return state, total_reward, terminated\n",
        "\n",
        "\n",
        "def explore_from_cell(env, trajectory, k_steps):\n",
        "    \"\"\"\n",
        "    Explore from a cell by taking k random actions.\n",
        "    \n",
        "    This is the \"explore\" phase that discovers new cells.\n",
        "    \n",
        "    Args:\n",
        "        env: The environment\n",
        "        trajectory: Actions to reach the starting cell\n",
        "        k_steps: Number of random exploratory steps\n",
        "        \n",
        "    Returns:\n",
        "        new_cells: Dictionary of {cell: (trajectory, reward)} for newly discovered cells\n",
        "    \"\"\"\n",
        "    new_cells = {}\n",
        "    \n",
        "    # Return to the starting cell\n",
        "    state, reward_so_far, terminated = rollout_to_cell(env, trajectory)\n",
        "    \n",
        "    if terminated:\n",
        "        # Can't explore from a terminal state\n",
        "        return new_cells\n",
        "    \n",
        "    current_trajectory = trajectory.copy()\n",
        "    \n",
        "    # Take k random exploratory steps\n",
        "    for _ in range(k_steps):\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        current_trajectory.append(action)\n",
        "        reward_so_far += reward\n",
        "        \n",
        "        cell = get_cell(state)\n",
        "        \n",
        "        # Store this cell (will be filtered later if already in archive with better reward)\n",
        "        new_cells[cell] = (current_trajectory.copy(), reward_so_far)\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    return new_cells\n",
        "\n",
        "print(\"Core functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Main Go-Explore Algorithm (Phase 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def go_explore_phase1(env, max_iterations=1000, k_explore=10, target_reward=1.0):\n",
        "    \"\"\"\n",
        "    Go-Explore Phase 1: Explore Until Solved\n",
        "    \n",
        "    Maintains an archive of discovered cells and systematically explores from them.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        max_iterations: Maximum number of iterations\n",
        "        k_explore: Number of random exploratory steps per iteration\n",
        "        target_reward: Reward threshold to consider the problem \"solved\"\n",
        "        \n",
        "    Returns:\n",
        "        archive: Dictionary of discovered cells\n",
        "        history: Dictionary tracking exploration progress\n",
        "    \"\"\"\n",
        "    # Initialize archive with the starting state\n",
        "    initial_state, _ = env.reset()\n",
        "    initial_cell = get_cell(initial_state)\n",
        "    \n",
        "    # Archive structure: {cell: {'trajectory': [...], 'reward': float}}\n",
        "    archive = {\n",
        "        initial_cell: {\n",
        "            'trajectory': [],\n",
        "            'reward': 0.0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Track statistics for visualization\n",
        "    history = {\n",
        "        'iterations': [],\n",
        "        'cells_discovered': [],\n",
        "        'max_reward': [],\n",
        "        'solved_iteration': None\n",
        "    }\n",
        "    \n",
        "    solved = False\n",
        "    \n",
        "    print(\"Starting Go-Explore Phase 1...\")\n",
        "    print(f\"Initial cell: {initial_cell}\")\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        # Step 1: Select a cell from the archive (random selection)\n",
        "        # More sophisticated versions might prioritize by reward or novelty\n",
        "        cell = random.choice(list(archive.keys()))\n",
        "        trajectory = archive[cell]['trajectory']\n",
        "        \n",
        "        # Step 2: Return to that cell and explore from it\n",
        "        new_cells = explore_from_cell(env, trajectory, k_explore)\n",
        "        \n",
        "        # Step 3: Update archive with newly discovered cells\n",
        "        for new_cell, (new_trajectory, new_reward) in new_cells.items():\n",
        "            # Only add/update if this is a new cell or we found a better trajectory\n",
        "            if new_cell not in archive or new_reward > archive[new_cell]['reward']:\n",
        "                archive[new_cell] = {\n",
        "                    'trajectory': new_trajectory,\n",
        "                    'reward': new_reward\n",
        "                }\n",
        "                \n",
        "                # Check if we've solved the problem\n",
        "                if new_reward >= target_reward and not solved:\n",
        "                    solved = True\n",
        "                    history['solved_iteration'] = iteration\n",
        "                    print(f\"\\nSOLVED at iteration {iteration}!\")\n",
        "                    print(f\"Solution trajectory length: {len(new_trajectory)}\")\n",
        "                    print(f\"Solution trajectory: {new_trajectory}\")\n",
        "        \n",
        "        # Record statistics\n",
        "        history['iterations'].append(iteration)\n",
        "        history['cells_discovered'].append(len(archive))\n",
        "        history['max_reward'].append(max(cell_data['reward'] for cell_data in archive.values()))\n",
        "        \n",
        "        # Progress reporting\n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration}: {len(archive)} cells discovered, \"\n",
        "                  f\"max reward: {history['max_reward'][-1]:.2f}\")\n",
        "        \n",
        "        # Early stopping if solved\n",
        "        if solved and iteration > history['solved_iteration'] + 50:\n",
        "            print(f\"\\nStopping after {iteration} iterations (problem solved).\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nExploration complete!\")\n",
        "    print(f\"Total cells discovered: {len(archive)}\")\n",
        "    print(f\"Final max reward: {max(cell_data['reward'] for cell_data in archive.values()):.2f}\")\n",
        "    \n",
        "    return archive, history\n",
        "\n",
        "print(\"Go-Explore algorithm defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Run the Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Go-Explore Phase 1\n",
        "# Using more iterations for the larger 8x8 environment\n",
        "archive, history = go_explore_phase1(\n",
        "    env=env,\n",
        "    max_iterations=2000,\n",
        "    k_explore=10,\n",
        "    target_reward=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results and Visualization\n",
        "\n",
        "### 5.1 Exploration Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Cells Discovered Over Time\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['iterations'], history['cells_discovered'], linewidth=2, color='steelblue')\n",
        "if history['solved_iteration'] is not None:\n",
        "    plt.axvline(x=history['solved_iteration'], color='green', linestyle='--', \n",
        "                label=f\"Solved at iteration {history['solved_iteration']}\")\n",
        "    plt.legend()\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Number of Unique Cells Discovered', fontsize=12)\n",
        "plt.title('Go-Explore: Exploration Progress', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Maximum Reward Over Time\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['iterations'], history['max_reward'], linewidth=2, color='coral')\n",
        "plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Target Reward (1.0)')\n",
        "if history['solved_iteration'] is not None:\n",
        "    plt.axvline(x=history['solved_iteration'], color='green', linestyle='--', \n",
        "                label=f\"Solved at iteration {history['solved_iteration']}\")\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Maximum Reward Found', fontsize=12)\n",
        "plt.title('Go-Explore: Best Reward Over Time', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Left plot: Shows how Go-Explore systematically discovers new cells over time.\")\n",
        "print(\"- Right plot: Tracks the best reward found so far. Once it reaches 1.0, we've found the goal.\")\n",
        "print(\"- The steady growth demonstrates that Go-Explore avoids detachmentâ€”it keeps expanding its frontier.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Archive Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the archive contents\n",
        "print(f\"Archive Statistics:\")\n",
        "print(f\"  Total unique cells discovered: {len(archive)}\")\n",
        "print(f\"  Cells by reward:\")\n",
        "\n",
        "# Group cells by reward\n",
        "reward_counts = defaultdict(int)\n",
        "for cell_data in archive.values():\n",
        "    reward_counts[cell_data['reward']] += 1\n",
        "\n",
        "for reward in sorted(reward_counts.keys(), reverse=True):\n",
        "    print(f\"    Reward {reward:.1f}: {reward_counts[reward]} cells\")\n",
        "\n",
        "# Find the best trajectories\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Top 5 Trajectories (by reward):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sorted_archive = sorted(archive.items(), key=lambda x: x[1]['reward'], reverse=True)\n",
        "\n",
        "action_names = {0: 'Left', 1: 'Down', 2: 'Right', 3: 'Up'}\n",
        "\n",
        "for i, (cell, data) in enumerate(sorted_archive[:5]):\n",
        "    traj_str = ' -> '.join([action_names[a] for a in data['trajectory']])\n",
        "    if not traj_str:\n",
        "        traj_str = \"(start state)\"\n",
        "    print(f\"\\n{i+1}. Cell {cell}: Reward = {data['reward']:.2f}\")\n",
        "    print(f\"   Trajectory length: {len(data['trajectory'])}\")\n",
        "    print(f\"   Actions: {traj_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Visualizing Discovered Cells on the Grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize which cells were discovered\n",
        "# FrozenLake 8x8 has 64 cells (0-63)\n",
        "grid_size = 8\n",
        "\n",
        "# Create a grid showing discovered cells\n",
        "discovered_grid = np.zeros((grid_size, grid_size))\n",
        "reward_grid = np.zeros((grid_size, grid_size))\n",
        "\n",
        "for cell, data in archive.items():\n",
        "    row = cell // grid_size\n",
        "    col = cell % grid_size\n",
        "    discovered_grid[row, col] = 1\n",
        "    reward_grid[row, col] = data['reward']\n",
        "\n",
        "# Plot the discovery grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Binary discovery map\n",
        "im1 = axes[0].imshow(discovered_grid, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "axes[0].set_title('Cells Discovered by Go-Explore', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Column')\n",
        "axes[0].set_ylabel('Row')\n",
        "\n",
        "# Add cell numbers (smaller font for 8x8 grid)\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        cell_num = i * grid_size + j\n",
        "        color = 'white' if discovered_grid[i, j] > 0.5 else 'black'\n",
        "        axes[0].text(j, i, str(cell_num), ha='center', va='center', \n",
        "                    color=color, fontweight='bold', fontsize=7)\n",
        "\n",
        "plt.colorbar(im1, ax=axes[0], label='Discovered (1) / Not Discovered (0)')\n",
        "\n",
        "# Plot 2: Reward heatmap\n",
        "im2 = axes[1].imshow(reward_grid, cmap='viridis')\n",
        "axes[1].set_title('Reward Heatmap', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Column')\n",
        "axes[1].set_ylabel('Row')\n",
        "\n",
        "# Add cell numbers and rewards (smaller font for 8x8 grid)\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        cell_num = i * grid_size + j\n",
        "        reward = reward_grid[i, j]\n",
        "        # Only show cell number (reward would be too cluttered)\n",
        "        axes[1].text(j, i, f\"{cell_num}\", \n",
        "                    ha='center', va='center', color='white', \n",
        "                    fontweight='bold', fontsize=6)\n",
        "\n",
        "plt.colorbar(im2, ax=axes[1], label='Cumulative Reward')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Left plot: Green cells were discovered by Go-Explore, red cells were not.\")\n",
        "print(\"- Right plot: Shows the maximum reward achieved when reaching each cell.\")\n",
        "print(\"- Go-Explore systematically explores the reachable state space.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Comparison: Go-Explore vs Pure Random Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_exploration(env, num_episodes=100, max_steps=50):\n",
        "    \"\"\"\n",
        "    Baseline: Pure random exploration without archive or systematic return.\n",
        "    \n",
        "    This demonstrates what Go-Explore improves upon.\n",
        "    \"\"\"\n",
        "    cells_discovered = set()\n",
        "    cells_history = []\n",
        "    max_reward = 0.0\n",
        "    solved = False\n",
        "    solved_episode = None\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        cells_discovered.add(get_cell(state))\n",
        "        total_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            cells_discovered.add(get_cell(state))\n",
        "            \n",
        "            if reward > 0:\n",
        "                max_reward = max(max_reward, total_reward)\n",
        "                if total_reward >= 1.0 and not solved:\n",
        "                    solved = True\n",
        "                    solved_episode = episode\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        cells_history.append(len(cells_discovered))\n",
        "    \n",
        "    return len(cells_discovered), max_reward, solved, solved_episode, cells_history\n",
        "\n",
        "\n",
        "# Run random exploration for comparison\n",
        "print(\"Running pure random exploration (baseline)...\")\n",
        "random_cells, random_max_reward, random_solved, random_solved_ep, random_history = random_exploration(\n",
        "    env, num_episodes=2000, max_steps=50\n",
        ")\n",
        "\n",
        "print(f\"\\nComparison Results:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Go-Explore (Phase 1):\")\n",
        "print(f\"  Cells discovered: {len(archive)}\")\n",
        "print(f\"  Max reward: {max(data['reward'] for data in archive.values()):.2f}\")\n",
        "print(f\"  Problem solved: {history['solved_iteration'] is not None}\")\n",
        "if history['solved_iteration'] is not None:\n",
        "    print(f\"  Iterations to solve: {history['solved_iteration']}\")\n",
        "\n",
        "print(f\"\\nPure Random Exploration:\")\n",
        "print(f\"  Cells discovered: {random_cells}\")\n",
        "print(f\"  Max reward: {random_max_reward:.2f}\")\n",
        "print(f\"  Problem solved: {random_solved}\")\n",
        "if random_solved_ep is not None:\n",
        "    print(f\"  Episodes to solve: {random_solved_ep}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Key Insight:\")\n",
        "if history['solved_iteration'] is not None and random_solved_ep is not None:\n",
        "    efficiency_ratio = random_solved_ep / history['solved_iteration']\n",
        "    print(f\"Go-Explore solved the problem in {history['solved_iteration']} iterations,\")\n",
        "    print(f\"while random exploration needed {random_solved_ep} episodes.\")\n",
        "    print(f\"Go-Explore is {efficiency_ratio:.1f}x more efficient!\")\n",
        "    print(\"\\nThis demonstrates Go-Explore's key advantage: SYSTEMATIC exploration.\")\n",
        "    print(\"By maintaining an archive and returning to promising states,\")\n",
        "    print(\"Go-Explore avoids redundant exploration and finds solutions faster.\")\n",
        "else:\n",
        "    print(\"Both methods successfully explored the environment, but Go-Explore\")\n",
        "    print(\"does so more systematically by maintaining an archive of discovered\")\n",
        "    print(\"states and deterministically returning to them for further exploration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "### What We Demonstrated\n",
        "\n",
        "This notebook implemented **Phase 1 of Go-Explore**, the \"Explore Until Solved\" phase, demonstrating the algorithm's core innovation in solving hard-exploration problems.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Archive-Based Exploration**: By maintaining an archive of visited states and their trajectories, Go-Explore can systematically explore the state space without forgetting how to return to promising regions.\n",
        "\n",
        "2. **Solving Detachment**: The algorithm stores exact trajectories to reach each cell, ensuring we never \"forget\" how to return to any discovered state. This is crucial for building on past progress.\n",
        "\n",
        "3. **Solving Derailment** (in deterministic settings): By returning deterministically to stored cells, we avoid the stochasticity that causes traditional algorithms to fail in reproducing successful behaviors.\n",
        "\n",
        "4. **Systematic vs Random**: Our comparison shows that Go-Explore's systematic \"return-then-explore\" approach discovers more states and achieves better rewards than pure random exploration, which suffers from detachment.\n",
        "\n",
        "### Phase 1 vs Phase 2\n",
        "\n",
        "**What we implemented (Phase 1):**\n",
        "- Archive maintenance\n",
        "- Deterministic return via trajectory replay\n",
        "- Random exploration from archived cells\n",
        "- Works in deterministic environments\n",
        "\n",
        "**What we omitted (Phase 2 - Robustification):**\n",
        "- Training a robust policy via imitation learning\n",
        "- Using the Backward Algorithm or PPO to learn from discovered trajectories\n",
        "- Handling stochastic environments\n",
        "- Deploying the policy in the real world\n",
        "\n",
        "Phase 2 would take the solution trajectory found by Phase 1 and train a neural network policy to robustly execute it even in stochastic environments. This could be implemented using behavioral cloning or policy optimization algorithms like PPO.\n",
        "\n",
        "### Limitations of This Implementation\n",
        "\n",
        "1. **Deterministic Requirement**: Phase 1 only works in deterministic environments. In stochastic settings, trajectory replay would not reliably return to the same cell.\n",
        "\n",
        "2. **Simple State Abstraction**: We used the raw grid position as our cell. More complex environments (e.g., images) would need sophisticated abstraction functions.\n",
        "\n",
        "3. **Random Cell Selection**: We randomly select cells from the archive. The paper suggests more sophisticated heuristics (e.g., prioritizing high-reward or less-visited cells).\n",
        "\n",
        "4. **No Robustification**: The trajectories found are brittleâ€”any environmental stochasticity would break them. Phase 2 addresses this.\n",
        "\n",
        "### References\n",
        "\n",
        "Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019). Go-Explore: A New Approach for Hard-Exploration Problems. arXiv preprint arXiv:1901.10995.\n",
        "\n",
        "Link: https://huggingface.co/papers/1901.10995\n",
        "\n",
        "---\n",
        "\n",
        "**End of Notebook**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
