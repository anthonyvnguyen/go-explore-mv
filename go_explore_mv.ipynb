{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Go-Explore: Phase 1 Implementation\n",
        "## A New Approach for Hard-Exploration Problems\n",
        "\n",
        "**Paper Reference:**  \n",
        "Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019).  \n",
        "*Go-Explore: A New Approach for Hard-Exploration Problems*  \n",
        "arXiv preprint arXiv:1901.10995\n",
        "\n",
        "**Paper Link:** https://huggingface.co/papers/1901.10995\n",
        "\n",
        "**Implementation:** Phase 1 (\"Explore Until Solved\") only  \n",
        "**Environment:** FrozenLake-v1 (deterministic, is_slippery=False)  \n",
        "**Target:** Graduate-level Reinforcement Learning midterm project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction and Summary\n",
        "\n",
        "### What is Go-Explore?\n",
        "\n",
        "Go-Explore is a novel exploration algorithm designed to solve \"hard-exploration\" problems in reinforcement learning. Traditional RL algorithms often struggle in sparse-reward environments due to two key failure modes:\n",
        "\n",
        "1. **Detachment**: The agent forgets how to return to promising states after exploring further.\n",
        "2. **Derailment**: Small stochastic perturbations cause the agent to deviate from promising trajectories, making it difficult to reproduce successful behaviors.\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "Go-Explore addresses these issues through a simple yet powerful principle: **\"Remember promising states and systematically return to them to explore further.\"**\n",
        "\n",
        "The algorithm maintains an **archive** of visited states (represented as abstract \"cells\") along with the trajectories needed to reach them. It then:\n",
        "1. Selects a promising cell from the archive\n",
        "2. **Returns** to that cell deterministically (solving detachment)\n",
        "3. **Explores** from that cell with random actions\n",
        "4. Adds any newly discovered cells to the archive\n",
        "\n",
        "### Two-Phase Approach\n",
        "\n",
        "- **Phase 1 (\"Explore Until Solved\")**: Use the archive-based exploration to find a solution in a deterministic environment\n",
        "- **Phase 2 (\"Robustification\")**: Train a robust policy via imitation learning (not implemented here)\n",
        "\n",
        "### This Notebook\n",
        "\n",
        "This notebook implements **Phase 1 only**, demonstrating the core exploration mechanism of Go-Explore in the simple FrozenLake environment. We show how the \"return-then-explore\" strategy systematically discovers all reachable states, solving the exploration problem that stymies standard random exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install gymnasium numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Algorithm Explanation: Phase 1 (\"Explore Until Solved\")\n",
        "\n",
        "### Key Components\n",
        "\n",
        "#### 3.1 State Abstraction: Cells\n",
        "\n",
        "Go-Explore groups similar states into abstract **cells**. In our FrozenLake implementation, each grid position naturally corresponds to a unique cell (the agent's position). This abstraction:\n",
        "- Reduces memory requirements\n",
        "- Makes the archive more manageable\n",
        "- Focuses exploration on meaningfully different states\n",
        "\n",
        "#### 3.2 The Archive\n",
        "\n",
        "The archive is the heart of Go-Explore. It stores:\n",
        "- **Cell representation**: An abstract representation of the state\n",
        "- **Trajectory**: The sequence of actions needed to reach that cell from the start\n",
        "- **Reward**: The cumulative reward obtained along that trajectory\n",
        "\n",
        "The archive grows as we discover new cells during exploration.\n",
        "\n",
        "#### 3.3 Return-Then-Explore\n",
        "\n",
        "The algorithm follows a simple loop:\n",
        "\n",
        "1. **Select**: Choose a cell from the archive (randomly or by heuristic)\n",
        "2. **Return**: Deterministically execute the stored trajectory to reach that cell\n",
        "3. **Explore**: Take K random exploratory actions from that cell\n",
        "4. **Update**: Add any newly discovered cells to the archive\n",
        "\n",
        "This approach solves both failure modes:\n",
        "- **Detachment**: We never \"forget\" how to return to promising statesâ€”the trajectory is stored\n",
        "- **Derailment**: We return deterministically (no stochasticity), ensuring we reliably reach the chosen cell\n",
        "\n",
        "#### 3.4 Why This Works\n",
        "\n",
        "Traditional random exploration struggles because:\n",
        "- Randomly revisiting a specific state is exponentially unlikely in large state spaces\n",
        "- Once we've moved away from a promising state, we rarely return to it\n",
        "\n",
        "Go-Explore systematically revisits all discovered states, ensuring comprehensive exploration without relying on rare random events.\n",
        "\n",
        "### Pseudo-code\n",
        "\n",
        "```\n",
        "Initialize archive with starting state\n",
        "while not solved:\n",
        "    cell = select_cell_from_archive()\n",
        "    return_to_cell(cell)  # Execute stored trajectory\n",
        "    for k in range(K):\n",
        "        action = random_action()\n",
        "        state, reward = step(action)\n",
        "        cell_new = get_cell(state)\n",
        "        if cell_new not in archive or reward > archive[cell_new].reward:\n",
        "            add_to_archive(cell_new, trajectory, reward)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implementation\n",
        "\n",
        "### 4.1 Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FrozenLake environment with deterministic dynamics\n",
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode=None)\n",
        "\n",
        "print(f\"Environment: {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Actions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
        "print(\"\\nFrozenLake Map:\")\n",
        "print(\"S = Start, F = Frozen (safe), H = Hole (terminal), G = Goal (terminal)\")\n",
        "print(env.unwrapped.desc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Core Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cell(state):\n",
        "    \"\"\"\n",
        "    State abstraction function: converts raw state to a cell representation.\n",
        "    \n",
        "    In FrozenLake, the state is already an integer representing grid position,\n",
        "    so we can use it directly as our cell. In more complex environments,\n",
        "    this might involve downsampling images or discretizing continuous states.\n",
        "    \n",
        "    Args:\n",
        "        state: The raw environment state\n",
        "        \n",
        "    Returns:\n",
        "        cell: Abstract cell representation\n",
        "    \"\"\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def rollout_to_cell(env, trajectory):\n",
        "    \"\"\"\n",
        "    Deterministically return to a cell by executing the stored trajectory.\n",
        "    \n",
        "    This is the \"return\" phase that solves the detachment problem.\n",
        "    Since our environment is deterministic (is_slippery=False), replaying\n",
        "    the same actions always reaches the same state.\n",
        "    \n",
        "    Args:\n",
        "        env: The environment\n",
        "        trajectory: List of actions to execute\n",
        "        \n",
        "    Returns:\n",
        "        state: The final state after executing the trajectory\n",
        "        total_reward: Cumulative reward obtained\n",
        "        terminated: Whether the episode terminated\n",
        "    \"\"\"\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    terminated = False\n",
        "    \n",
        "    for action in trajectory:\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    return state, total_reward, terminated\n",
        "\n",
        "\n",
        "def explore_from_cell(env, trajectory, k_steps):\n",
        "    \"\"\"\n",
        "    Explore from a cell by taking k random actions.\n",
        "    \n",
        "    This is the \"explore\" phase that discovers new cells.\n",
        "    \n",
        "    Args:\n",
        "        env: The environment\n",
        "        trajectory: Actions to reach the starting cell\n",
        "        k_steps: Number of random exploratory steps\n",
        "        \n",
        "    Returns:\n",
        "        new_cells: Dictionary of {cell: (trajectory, reward)} for newly discovered cells\n",
        "    \"\"\"\n",
        "    new_cells = {}\n",
        "    \n",
        "    # Return to the starting cell\n",
        "    state, reward_so_far, terminated = rollout_to_cell(env, trajectory)\n",
        "    \n",
        "    if terminated:\n",
        "        # Can't explore from a terminal state\n",
        "        return new_cells\n",
        "    \n",
        "    current_trajectory = trajectory.copy()\n",
        "    \n",
        "    # Take k random exploratory steps\n",
        "    for _ in range(k_steps):\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        current_trajectory.append(action)\n",
        "        reward_so_far += reward\n",
        "        \n",
        "        cell = get_cell(state)\n",
        "        \n",
        "        # Store this cell (will be filtered later if already in archive with better reward)\n",
        "        new_cells[cell] = (current_trajectory.copy(), reward_so_far)\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    return new_cells\n",
        "\n",
        "print(\"Core functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Main Go-Explore Algorithm (Phase 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def go_explore_phase1(env, max_iterations=1000, k_explore=10, target_reward=1.0):\n",
        "    \"\"\"\n",
        "    Go-Explore Phase 1: Explore Until Solved\n",
        "    \n",
        "    Maintains an archive of discovered cells and systematically explores from them.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        max_iterations: Maximum number of iterations\n",
        "        k_explore: Number of random exploratory steps per iteration\n",
        "        target_reward: Reward threshold to consider the problem \"solved\"\n",
        "        \n",
        "    Returns:\n",
        "        archive: Dictionary of discovered cells\n",
        "        history: Dictionary tracking exploration progress\n",
        "    \"\"\"\n",
        "    # Initialize archive with the starting state\n",
        "    initial_state, _ = env.reset()\n",
        "    initial_cell = get_cell(initial_state)\n",
        "    \n",
        "    # Archive structure: {cell: {'trajectory': [...], 'reward': float}}\n",
        "    archive = {\n",
        "        initial_cell: {\n",
        "            'trajectory': [],\n",
        "            'reward': 0.0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Track statistics for visualization\n",
        "    history = {\n",
        "        'iterations': [],\n",
        "        'cells_discovered': [],\n",
        "        'max_reward': [],\n",
        "        'solved_iteration': None\n",
        "    }\n",
        "    \n",
        "    solved = False\n",
        "    \n",
        "    print(\"Starting Go-Explore Phase 1...\")\n",
        "    print(f\"Initial cell: {initial_cell}\")\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        # Step 1: Select a cell from the archive (random selection)\n",
        "        # More sophisticated versions might prioritize by reward or novelty\n",
        "        cell = random.choice(list(archive.keys()))\n",
        "        trajectory = archive[cell]['trajectory']\n",
        "        \n",
        "        # Step 2: Return to that cell and explore from it\n",
        "        new_cells = explore_from_cell(env, trajectory, k_explore)\n",
        "        \n",
        "        # Step 3: Update archive with newly discovered cells\n",
        "        for new_cell, (new_trajectory, new_reward) in new_cells.items():\n",
        "            # Only add/update if this is a new cell or we found a better trajectory\n",
        "            if new_cell not in archive or new_reward > archive[new_cell]['reward']:\n",
        "                archive[new_cell] = {\n",
        "                    'trajectory': new_trajectory,\n",
        "                    'reward': new_reward\n",
        "                }\n",
        "                \n",
        "                # Check if we've solved the problem\n",
        "                if new_reward >= target_reward and not solved:\n",
        "                    solved = True\n",
        "                    history['solved_iteration'] = iteration\n",
        "                    print(f\"\\nSOLVED at iteration {iteration}!\")\n",
        "                    print(f\"Solution trajectory length: {len(new_trajectory)}\")\n",
        "                    print(f\"Solution trajectory: {new_trajectory}\")\n",
        "        \n",
        "        # Record statistics\n",
        "        history['iterations'].append(iteration)\n",
        "        history['cells_discovered'].append(len(archive))\n",
        "        history['max_reward'].append(max(cell_data['reward'] for cell_data in archive.values()))\n",
        "        \n",
        "        # Progress reporting\n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration}: {len(archive)} cells discovered, \"\n",
        "                  f\"max reward: {history['max_reward'][-1]:.2f}\")\n",
        "        \n",
        "        # Early stopping if solved\n",
        "        if solved and iteration > history['solved_iteration'] + 50:\n",
        "            print(f\"\\nStopping after {iteration} iterations (problem solved).\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nExploration complete!\")\n",
        "    print(f\"Total cells discovered: {len(archive)}\")\n",
        "    print(f\"Final max reward: {max(cell_data['reward'] for cell_data in archive.values()):.2f}\")\n",
        "    \n",
        "    return archive, history\n",
        "\n",
        "print(\"Go-Explore algorithm defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Run the Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Go-Explore Phase 1\n",
        "# Using more iterations for the larger 8x8 environment\n",
        "archive, history = go_explore_phase1(\n",
        "    env=env,\n",
        "    max_iterations=2000,\n",
        "    k_explore=10,\n",
        "    target_reward=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results and Visualization\n",
        "\n",
        "### 5.1 Exploration Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Cells Discovered Over Time\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['iterations'], history['cells_discovered'], linewidth=2, color='steelblue')\n",
        "if history['solved_iteration'] is not None:\n",
        "    plt.axvline(x=history['solved_iteration'], color='green', linestyle='--', \n",
        "                label=f\"Solved at iteration {history['solved_iteration']}\")\n",
        "    plt.legend()\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Number of Unique Cells Discovered', fontsize=12)\n",
        "plt.title('Go-Explore: Exploration Progress', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Maximum Reward Over Time\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['iterations'], history['max_reward'], linewidth=2, color='coral')\n",
        "plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Target Reward (1.0)')\n",
        "if history['solved_iteration'] is not None:\n",
        "    plt.axvline(x=history['solved_iteration'], color='green', linestyle='--', \n",
        "                label=f\"Solved at iteration {history['solved_iteration']}\")\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Maximum Reward Found', fontsize=12)\n",
        "plt.title('Go-Explore: Best Reward Over Time', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Left plot: Shows how Go-Explore systematically discovers new cells over time.\")\n",
        "print(\"- Right plot: Tracks the best reward found so far. Once it reaches 1.0, we've found the goal.\")\n",
        "print(\"- The steady growth demonstrates that Go-Explore avoids detachmentâ€”it keeps expanding its frontier.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Archive Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the archive contents\n",
        "print(f\"Archive Statistics:\")\n",
        "print(f\"  Total unique cells discovered: {len(archive)}\")\n",
        "print(f\"  Cells by reward:\")\n",
        "\n",
        "# Group cells by reward\n",
        "reward_counts = defaultdict(int)\n",
        "for cell_data in archive.values():\n",
        "    reward_counts[cell_data['reward']] += 1\n",
        "\n",
        "for reward in sorted(reward_counts.keys(), reverse=True):\n",
        "    print(f\"    Reward {reward:.1f}: {reward_counts[reward]} cells\")\n",
        "\n",
        "# Find the best trajectories\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Top 5 Trajectories (by reward):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sorted_archive = sorted(archive.items(), key=lambda x: x[1]['reward'], reverse=True)\n",
        "\n",
        "action_names = {0: 'Left', 1: 'Down', 2: 'Right', 3: 'Up'}\n",
        "\n",
        "for i, (cell, data) in enumerate(sorted_archive[:5]):\n",
        "    traj_str = ' -> '.join([action_names[a] for a in data['trajectory']])\n",
        "    if not traj_str:\n",
        "        traj_str = \"(start state)\"\n",
        "    print(f\"\\n{i+1}. Cell {cell}: Reward = {data['reward']:.2f}\")\n",
        "    print(f\"   Trajectory length: {len(data['trajectory'])}\")\n",
        "    print(f\"   Actions: {traj_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Visualizing Discovered Cells on the Grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize which cells were discovered\n",
        "# FrozenLake 8x8 has 64 cells (0-63)\n",
        "grid_size = 8\n",
        "\n",
        "# Create a grid showing discovered cells\n",
        "discovered_grid = np.zeros((grid_size, grid_size))\n",
        "reward_grid = np.zeros((grid_size, grid_size))\n",
        "\n",
        "for cell, data in archive.items():\n",
        "    row = cell // grid_size\n",
        "    col = cell % grid_size\n",
        "    discovered_grid[row, col] = 1\n",
        "    reward_grid[row, col] = data['reward']\n",
        "\n",
        "# Plot the discovery grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Binary discovery map\n",
        "im1 = axes[0].imshow(discovered_grid, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "axes[0].set_title('Cells Discovered by Go-Explore', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Column')\n",
        "axes[0].set_ylabel('Row')\n",
        "\n",
        "# Add cell numbers (smaller font for 8x8 grid)\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        cell_num = i * grid_size + j\n",
        "        color = 'white' if discovered_grid[i, j] > 0.5 else 'black'\n",
        "        axes[0].text(j, i, str(cell_num), ha='center', va='center', \n",
        "                    color=color, fontweight='bold', fontsize=7)\n",
        "\n",
        "plt.colorbar(im1, ax=axes[0], label='Discovered (1) / Not Discovered (0)')\n",
        "\n",
        "# Plot 2: Reward heatmap\n",
        "im2 = axes[1].imshow(reward_grid, cmap='viridis')\n",
        "axes[1].set_title('Reward Heatmap', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Column')\n",
        "axes[1].set_ylabel('Row')\n",
        "\n",
        "# Add cell numbers and rewards (smaller font for 8x8 grid)\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        cell_num = i * grid_size + j\n",
        "        reward = reward_grid[i, j]\n",
        "        # Only show cell number (reward would be too cluttered)\n",
        "        axes[1].text(j, i, f\"{cell_num}\", \n",
        "                    ha='center', va='center', color='white', \n",
        "                    fontweight='bold', fontsize=6)\n",
        "\n",
        "plt.colorbar(im2, ax=axes[1], label='Cumulative Reward')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Left plot: Green cells were discovered by Go-Explore, red cells were not.\")\n",
        "print(\"- Right plot: Shows the maximum reward achieved when reaching each cell.\")\n",
        "print(\"- Go-Explore systematically explores the reachable state space.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Comparison: Go-Explore vs Pure Random Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_exploration(env, num_episodes=100, max_steps=50):\n",
        "    \"\"\"\n",
        "    Baseline: Pure random exploration without archive or systematic return.\n",
        "    \n",
        "    This demonstrates what Go-Explore improves upon.\n",
        "    \"\"\"\n",
        "    cells_discovered = set()\n",
        "    cells_history = []\n",
        "    max_reward = 0.0\n",
        "    solved = False\n",
        "    solved_episode = None\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        cells_discovered.add(get_cell(state))\n",
        "        total_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            cells_discovered.add(get_cell(state))\n",
        "            \n",
        "            if reward > 0:\n",
        "                max_reward = max(max_reward, total_reward)\n",
        "                if total_reward >= 1.0 and not solved:\n",
        "                    solved = True\n",
        "                    solved_episode = episode\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        cells_history.append(len(cells_discovered))\n",
        "    \n",
        "    return len(cells_discovered), max_reward, solved, solved_episode, cells_history\n",
        "\n",
        "\n",
        "# Run random exploration for comparison\n",
        "print(\"Running pure random exploration (baseline)...\")\n",
        "random_cells, random_max_reward, random_solved, random_solved_ep, random_history = random_exploration(\n",
        "    env, num_episodes=2000, max_steps=50\n",
        ")\n",
        "\n",
        "print(f\"\\nComparison Results:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Go-Explore (Phase 1):\")\n",
        "print(f\"  Cells discovered: {len(archive)}\")\n",
        "print(f\"  Max reward: {max(data['reward'] for data in archive.values()):.2f}\")\n",
        "print(f\"  Problem solved: {history['solved_iteration'] is not None}\")\n",
        "if history['solved_iteration'] is not None:\n",
        "    print(f\"  Iterations to solve: {history['solved_iteration']}\")\n",
        "\n",
        "print(f\"\\nPure Random Exploration:\")\n",
        "print(f\"  Cells discovered: {random_cells}\")\n",
        "print(f\"  Max reward: {random_max_reward:.2f}\")\n",
        "print(f\"  Problem solved: {random_solved}\")\n",
        "if random_solved_ep is not None:\n",
        "    print(f\"  Episodes to solve: {random_solved_ep}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Key Insight:\")\n",
        "if history['solved_iteration'] is not None and random_solved_ep is not None:\n",
        "    efficiency_ratio = random_solved_ep / history['solved_iteration']\n",
        "    print(f\"Go-Explore solved the problem in {history['solved_iteration']} iterations,\")\n",
        "    print(f\"while random exploration needed {random_solved_ep} episodes.\")\n",
        "    print(f\"Go-Explore is {efficiency_ratio:.1f}x more efficient!\")\n",
        "    print(\"\\nThis demonstrates Go-Explore's key advantage: SYSTEMATIC exploration.\")\n",
        "    print(\"By maintaining an archive and returning to promising states,\")\n",
        "    print(\"Go-Explore avoids redundant exploration and finds solutions faster.\")\n",
        "else:\n",
        "    print(\"Both methods successfully explored the environment, but Go-Explore\")\n",
        "    print(\"does so more systematically by maintaining an archive of discovered\")\n",
        "    print(\"states and deterministically returning to them for further exploration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "### What We Demonstrated\n",
        "\n",
        "This notebook implemented **Phase 1 of Go-Explore**, the \"Explore Until Solved\" phase, demonstrating the algorithm's core innovation in solving hard-exploration problems.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Archive-Based Exploration**: By maintaining an archive of visited states and their trajectories, Go-Explore can systematically explore the state space without forgetting how to return to promising regions.\n",
        "\n",
        "2. **Solving Detachment**: The algorithm stores exact trajectories to reach each cell, ensuring we never \"forget\" how to return to any discovered state. This is crucial for building on past progress.\n",
        "\n",
        "3. **Solving Derailment** (in deterministic settings): By returning deterministically to stored cells, we avoid the stochasticity that causes traditional algorithms to fail in reproducing successful behaviors.\n",
        "\n",
        "4. **Systematic vs Random**: Our comparison shows that Go-Explore's systematic \"return-then-explore\" approach discovers more states and achieves better rewards than pure random exploration, which suffers from detachment.\n",
        "\n",
        "### Phase 1 vs Phase 2\n",
        "\n",
        "**What we implemented (Phase 1):**\n",
        "- Archive maintenance\n",
        "- Deterministic return via trajectory replay\n",
        "- Random exploration from archived cells\n",
        "- Works in deterministic environments\n",
        "\n",
        "**What we omitted (Phase 2 - Robustification):**\n",
        "- Training a robust policy via imitation learning\n",
        "- Using the Backward Algorithm or PPO to learn from discovered trajectories\n",
        "- Handling stochastic environments\n",
        "- Deploying the policy in the real world\n",
        "\n",
        "Phase 2 would take the solution trajectory found by Phase 1 and train a neural network policy to robustly execute it even in stochastic environments. This could be implemented using behavioral cloning or policy optimization algorithms like PPO.\n",
        "\n",
        "### Limitations of This Implementation\n",
        "\n",
        "1. **Deterministic Requirement**: Phase 1 only works in deterministic environments. In stochastic settings, trajectory replay would not reliably return to the same cell.\n",
        "\n",
        "2. **Simple State Abstraction**: We used the raw grid position as our cell. More complex environments (e.g., images) would need sophisticated abstraction functions.\n",
        "\n",
        "3. **Random Cell Selection**: We randomly select cells from the archive. The paper suggests more sophisticated heuristics (e.g., prioritizing high-reward or less-visited cells).\n",
        "\n",
        "4. **No Robustification**: The trajectories found are brittleâ€”any environmental stochasticity would break them. Phase 2 addresses this.\n",
        "\n",
        "### References\n",
        "\n",
        "Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019). Go-Explore: A New Approach for Hard-Exploration Problems. arXiv preprint arXiv:1901.10995.\n",
        "\n",
        "Link: https://huggingface.co/papers/1901.10995\n",
        "\n",
        "---\n",
        "\n",
        "**End of Notebook**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Phase 2 â€” Robustification via PPO (Backward Algorithm)\n",
        "\n",
        "## Overview\n",
        "\n",
        "While Phase 1 successfully discovers a solution trajectory through systematic exploration, this trajectory is **brittle**: it only works in the deterministic environment and requires exact state-by-state replay. Any environmental stochasticity would break the solution.\n",
        "\n",
        "**Phase 2** addresses this limitation by training a **robust neural network policy** that can:\n",
        "1. Generalize across different starting conditions\n",
        "2. Handle environmental stochasticity\n",
        "3. Recover from small perturbations\n",
        "\n",
        "### The Go-Explore Phase 2 Approach\n",
        "\n",
        "According to the original paper (Ecoffet et al., 2019), Phase 2 uses:\n",
        "- **Imitation Learning**: Learn to reproduce the successful trajectories from the archive\n",
        "- **Policy Optimization**: Use PPO (Proximal Policy Optimization) to refine the policy\n",
        "- **Backward Algorithm**: A curriculum learning strategy that trains the policy starting from near the goal, then progressively moves the training start point earlier in the trajectory\n",
        "\n",
        "### Why the Backward Algorithm?\n",
        "\n",
        "Training a policy to solve the entire task from scratch is difficult due to:\n",
        "- **Sparse rewards**: Success is only achieved at the end\n",
        "- **Credit assignment**: Hard to determine which early actions led to success\n",
        "- **Exploration**: The policy must explore extensively to find the goal\n",
        "\n",
        "The **Backward Algorithm** solves this by:\n",
        "1. **Starting near the goal**: Initial training begins from states close to success (e.g., last 10% of the trajectory)\n",
        "2. **High initial success rate**: The policy quickly learns to reach the goal from nearby states\n",
        "3. **Progressive curriculum**: Once the policy achieves >90% success from the current starting point, we move the start earlier in the trajectory\n",
        "4. **Gradual difficulty increase**: This continues until the policy can solve the task from the initial state\n",
        "\n",
        "This curriculum ensures the policy always has a clear learning signal and builds competence incrementally.\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "**Original Go-Explore (Atari):**\n",
        "- CNN-based policy for pixel inputs\n",
        "- PPO with complex hyperparameters\n",
        "- Thousands of training episodes\n",
        "\n",
        "**Our Simplified Version (FrozenLake):**\n",
        "- MLP policy for discrete state inputs (one-hot encoded)\n",
        "- Standard PPO implementation\n",
        "- Fewer training episodes (simpler environment)\n",
        "\n",
        "**Why this matches Go-Explore's conceptual design:**\n",
        "- Both use PPO for robustification\n",
        "- Both employ the Backward Algorithm curriculum\n",
        "- Both aim to convert brittle trajectories into robust policies\n",
        "- The main difference is input representation (pixels vs discrete states)\n",
        "\n",
        "### References\n",
        "\n",
        "- **Original Paper**: Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019). *Go-Explore: A New Approach for Hard-Exploration Problems*. arXiv:1901.10995\n",
        "- **Paper Link**: https://huggingface.co/papers/1901.10995\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Phase 2 Setup: Dependencies and Best Trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch for Phase 2\n",
        "%pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Extract the best trajectory from Phase 1 archive\n",
        "best_cell = max(archive.items(), key=lambda x: x[1]['reward'])\n",
        "best_trajectory = best_cell[1]['trajectory']\n",
        "best_reward = best_cell[1]['reward']\n",
        "best_cell_id = best_cell[0]\n",
        "\n",
        "print(f\"\\nBest trajectory from Phase 1:\")\n",
        "print(f\"  Cell: {best_cell_id}\")\n",
        "print(f\"  Reward: {best_reward}\")\n",
        "print(f\"  Trajectory length: {len(best_trajectory)}\")\n",
        "print(f\"  Actions: {best_trajectory}\")\n",
        "\n",
        "action_names = {0: 'Left', 1: 'Down', 2: 'Right', 3: 'Up'}\n",
        "trajectory_str = ' â†’ '.join([action_names[a] for a in best_trajectory])\n",
        "print(f\"  Path: START â†’ {trajectory_str} â†’ GOAL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Policy-Value Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP-based Actor-Critic network for discrete state space.\n",
        "    \n",
        "    Architecture:\n",
        "    - Input: One-hot encoded state (64 dimensions for FrozenLake 8Ã—8)\n",
        "    - Hidden: 2 layers of 128 units with ReLU activation\n",
        "    - Output heads:\n",
        "        * Policy (Actor): Logits over 4 actions\n",
        "        * Value (Critic): Scalar state value estimate\n",
        "    \n",
        "    This is analogous to the CNN-based architecture used in the original\n",
        "    Go-Explore paper on Atari, adapted for discrete state spaces.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_states=64, num_actions=4, hidden_dim=128):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        \n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        # Shared layers\n",
        "        self.fc1 = nn.Linear(num_states, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "        # Policy head (actor)\n",
        "        self.policy_head = nn.Linear(hidden_dim, num_actions)\n",
        "        \n",
        "        # Value head (critic)\n",
        "        self.value_head = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            state: Tensor of shape (batch_size, num_states) - one-hot encoded states\n",
        "            \n",
        "        Returns:\n",
        "            policy_logits: Tensor of shape (batch_size, num_actions)\n",
        "            value: Tensor of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Shared layers with ReLU activation\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        # Policy and value outputs\n",
        "        policy_logits = self.policy_head(x)\n",
        "        value = self.value_head(x)\n",
        "        \n",
        "        return policy_logits, value\n",
        "    \n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"\n",
        "        Sample an action from the policy.\n",
        "        \n",
        "        Args:\n",
        "            state: Integer state ID\n",
        "            deterministic: If True, select argmax action; else sample from distribution\n",
        "            \n",
        "        Returns:\n",
        "            action: Selected action\n",
        "            log_prob: Log probability of the action\n",
        "            value: State value estimate\n",
        "        \"\"\"\n",
        "        # Convert state to one-hot tensor\n",
        "        state_onehot = torch.zeros(1, self.num_states, device=device)\n",
        "        state_onehot[0, state] = 1.0\n",
        "        \n",
        "        # Forward pass\n",
        "        policy_logits, value = self.forward(state_onehot)\n",
        "        \n",
        "        # Create categorical distribution\n",
        "        dist = Categorical(logits=policy_logits)\n",
        "        \n",
        "        # Sample or select greedy action\n",
        "        if deterministic:\n",
        "            action = torch.argmax(policy_logits, dim=1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "        \n",
        "        log_prob = dist.log_prob(action)\n",
        "        \n",
        "        return action.item(), log_prob, value.squeeze()\n",
        "\n",
        "# Initialize the network\n",
        "policy_network = ActorCriticNetwork(num_states=64, num_actions=4, hidden_dim=128).to(device)\n",
        "\n",
        "print(\"Actor-Critic Network Architecture:\")\n",
        "print(policy_network)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in policy_network.parameters()):,}\")\n",
        "\n",
        "# Test the network with a dummy state\n",
        "test_state = 0  # Starting state\n",
        "test_action, test_log_prob, test_value = policy_network.get_action(test_state)\n",
        "print(f\"\\nNetwork test (state {test_state}):\")\n",
        "print(f\"  Sampled action: {test_action} ({action_names[test_action]})\")\n",
        "print(f\"  Log probability: {test_log_prob.item():.4f}\")\n",
        "print(f\"  Value estimate: {test_value.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. PPO Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_trajectories(env, policy, num_episodes, start_index=0, reference_trajectory=None):\n",
        "    \"\"\"\n",
        "    Collect trajectories using the current policy.\n",
        "    \n",
        "    For the Backward Algorithm, we can start from a specific point in a reference trajectory\n",
        "    rather than from the environment's initial state.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        policy: Policy network\n",
        "        num_episodes: Number of episodes to collect\n",
        "        start_index: Index in reference trajectory to start from (0 = full episode)\n",
        "        reference_trajectory: The Phase 1 trajectory to use for initialization\n",
        "        \n",
        "    Returns:\n",
        "        batch_data: Dictionary containing states, actions, log_probs, rewards, values, dones\n",
        "    \"\"\"\n",
        "    states_list = []\n",
        "    actions_list = []\n",
        "    log_probs_list = []\n",
        "    rewards_list = []\n",
        "    values_list = []\n",
        "    dones_list = []\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        # Initialize from reference trajectory if using Backward Algorithm\n",
        "        if reference_trajectory and start_index > 0:\n",
        "            state, _, terminated = rollout_to_cell(env, reference_trajectory[:start_index])\n",
        "            if terminated:\n",
        "                # If we hit a terminal state, restart from beginning\n",
        "                state, _ = env.reset()\n",
        "        else:\n",
        "            state, _ = env.reset()\n",
        "        \n",
        "        episode_states = []\n",
        "        episode_actions = []\n",
        "        episode_log_probs = []\n",
        "        episode_rewards = []\n",
        "        episode_values = []\n",
        "        episode_dones = []\n",
        "        \n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 100  # Prevent infinite loops\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            # Get action from policy\n",
        "            action, log_prob, value = policy.get_action(state)\n",
        "            \n",
        "            # Take step in environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_log_probs.append(log_prob)\n",
        "            episode_rewards.append(reward)\n",
        "            episode_values.append(value)\n",
        "            episode_dones.append(done)\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        # Add episode data to batch\n",
        "        states_list.extend(episode_states)\n",
        "        actions_list.extend(episode_actions)\n",
        "        log_probs_list.extend(episode_log_probs)\n",
        "        rewards_list.extend(episode_rewards)\n",
        "        values_list.extend(episode_values)\n",
        "        dones_list.extend(episode_dones)\n",
        "    \n",
        "    return {\n",
        "        'states': states_list,\n",
        "        'actions': actions_list,\n",
        "        'log_probs': log_probs_list,\n",
        "        'rewards': rewards_list,\n",
        "        'values': values_list,\n",
        "        'dones': dones_list\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    \"\"\"\n",
        "    Compute Generalized Advantage Estimation (GAE).\n",
        "    \n",
        "    Args:\n",
        "        rewards: List of rewards\n",
        "        values: List of value estimates\n",
        "        dones: List of done flags\n",
        "        gamma: Discount factor\n",
        "        lam: GAE lambda parameter\n",
        "        \n",
        "    Returns:\n",
        "        advantages: Computed advantages\n",
        "        returns: Discounted returns (targets for value function)\n",
        "    \"\"\"\n",
        "    advantages = []\n",
        "    returns = []\n",
        "    \n",
        "    gae = 0\n",
        "    next_value = 0\n",
        "    \n",
        "    # Process in reverse order\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        if dones[t]:\n",
        "            next_value = 0\n",
        "            gae = 0\n",
        "        \n",
        "        # TD error\n",
        "        delta = rewards[t] + gamma * next_value - values[t].item()\n",
        "        \n",
        "        # GAE\n",
        "        gae = delta + gamma * lam * gae\n",
        "        \n",
        "        advantages.insert(0, gae)\n",
        "        returns.insert(0, gae + values[t].item())\n",
        "        \n",
        "        next_value = values[t].item()\n",
        "    \n",
        "    return advantages, returns\n",
        "\n",
        "\n",
        "def ppo_update(policy, optimizer, batch_data, advantages, returns, \n",
        "               clip_range=0.2, value_coef=0.5, entropy_coef=0.01, epochs=4):\n",
        "    \"\"\"\n",
        "    Perform PPO update on collected batch.\n",
        "    \n",
        "    Args:\n",
        "        policy: Policy network\n",
        "        optimizer: Optimizer\n",
        "        batch_data: Dictionary of trajectory data\n",
        "        advantages: Computed advantages\n",
        "        returns: Target returns for value function\n",
        "        clip_range: PPO clipping parameter (epsilon)\n",
        "        value_coef: Coefficient for value loss\n",
        "        entropy_coef: Coefficient for entropy bonus\n",
        "        epochs: Number of optimization epochs over the batch\n",
        "        \n",
        "    Returns:\n",
        "        losses: Dictionary of loss values\n",
        "    \"\"\"\n",
        "    states = batch_data['states']\n",
        "    actions = batch_data['actions']\n",
        "    old_log_probs = batch_data['log_probs']\n",
        "    \n",
        "    # Convert to tensors\n",
        "    advantages_tensor = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
        "    returns_tensor = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    actions_tensor = torch.tensor(actions, dtype=torch.long, device=device)\n",
        "    old_log_probs_tensor = torch.stack(old_log_probs).detach()\n",
        "    \n",
        "    # Normalize advantages\n",
        "    advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "    \n",
        "    total_policy_loss = 0\n",
        "    total_value_loss = 0\n",
        "    total_entropy = 0\n",
        "    \n",
        "    # Multiple epochs over the same batch (PPO style)\n",
        "    for epoch in range(epochs):\n",
        "        # Convert states to one-hot\n",
        "        states_onehot = torch.zeros(len(states), policy.num_states, device=device)\n",
        "        for i, s in enumerate(states):\n",
        "            states_onehot[i, s] = 1.0\n",
        "        \n",
        "        # Forward pass\n",
        "        policy_logits, values = policy(states_onehot)\n",
        "        values = values.squeeze()\n",
        "        \n",
        "        # Compute action probabilities and entropy\n",
        "        dist = Categorical(logits=policy_logits)\n",
        "        log_probs = dist.log_prob(actions_tensor)\n",
        "        entropy = dist.entropy().mean()\n",
        "        \n",
        "        # PPO clipped objective\n",
        "        ratio = torch.exp(log_probs - old_log_probs_tensor)\n",
        "        clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
        "        \n",
        "        policy_loss = -torch.min(ratio * advantages_tensor, \n",
        "                                  clipped_ratio * advantages_tensor).mean()\n",
        "        \n",
        "        # Value loss\n",
        "        value_loss = F.mse_loss(values, returns_tensor)\n",
        "        \n",
        "        # Total loss\n",
        "        loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
        "        \n",
        "        # Optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_policy_loss += policy_loss.item()\n",
        "        total_value_loss += value_loss.item()\n",
        "        total_entropy += entropy.item()\n",
        "    \n",
        "    return {\n",
        "        'policy_loss': total_policy_loss / epochs,\n",
        "        'value_loss': total_value_loss / epochs,\n",
        "        'entropy': total_entropy / epochs\n",
        "    }\n",
        "\n",
        "print(\"PPO training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Backward Algorithm Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(env, policy, num_episodes=100, start_index=0, reference_trajectory=None, deterministic=True):\n",
        "    \"\"\"\n",
        "    Evaluate policy performance.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        policy: Policy network\n",
        "        num_episodes: Number of evaluation episodes\n",
        "        start_index: Starting point in reference trajectory (for curriculum evaluation)\n",
        "        reference_trajectory: Reference trajectory for initialization\n",
        "        deterministic: Whether to use deterministic (greedy) action selection\n",
        "        \n",
        "    Returns:\n",
        "        success_rate: Fraction of episodes that reached the goal\n",
        "        avg_reward: Average cumulative reward\n",
        "    \"\"\"\n",
        "    successes = 0\n",
        "    total_reward = 0\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        # Initialize from reference trajectory if needed\n",
        "        if reference_trajectory and start_index > 0:\n",
        "            state, _, terminated = rollout_to_cell(env, reference_trajectory[:start_index])\n",
        "            if terminated:\n",
        "                state, _ = env.reset()\n",
        "        else:\n",
        "            state, _ = env.reset()\n",
        "        \n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 100\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _, _ = policy.get_action(state, deterministic=deterministic)\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            done = terminated or truncated\n",
        "            steps += 1\n",
        "        \n",
        "        if episode_reward >= 1.0:  # Goal reached\n",
        "            successes += 1\n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    success_rate = successes / num_episodes\n",
        "    avg_reward = total_reward / num_episodes\n",
        "    \n",
        "    return success_rate, avg_reward\n",
        "\n",
        "print(\"Evaluation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_algorithm_ppo(env, policy, reference_trajectory, \n",
        "                           learning_rate=3e-4, gamma=0.99, \n",
        "                           success_threshold=0.9, max_iterations_per_phase=50,\n",
        "                           episodes_per_iteration=20):\n",
        "    \"\"\"\n",
        "    Backward Algorithm: Train policy using PPO with curriculum learning.\n",
        "    \n",
        "    The algorithm starts training from near the end of the reference trajectory\n",
        "    and progressively moves the starting point earlier as the policy improves.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        policy: Policy network\n",
        "        reference_trajectory: Best trajectory from Phase 1\n",
        "        learning_rate: Learning rate for Adam optimizer\n",
        "        gamma: Discount factor\n",
        "        success_threshold: Success rate required to move to earlier starting point\n",
        "        max_iterations_per_phase: Max training iterations per curriculum phase\n",
        "        episodes_per_iteration: Number of episodes to collect per iteration\n",
        "        \n",
        "    Returns:\n",
        "        training_history: Dictionary tracking training progress\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Determine curriculum schedule\n",
        "    traj_length = len(reference_trajectory)\n",
        "    \n",
        "    # Start from 90% of the trajectory, then move to 70%, 50%, 30%, 0%\n",
        "    curriculum_stages = [\n",
        "        int(0.9 * traj_length),\n",
        "        int(0.7 * traj_length),\n",
        "        int(0.5 * traj_length),\n",
        "        int(0.3 * traj_length),\n",
        "        0  # Full episode\n",
        "    ]\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"BACKWARD ALGORITHM TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Reference trajectory length: {traj_length} steps\")\n",
        "    print(f\"Curriculum stages: {curriculum_stages}\")\n",
        "    print(f\"Success threshold: {success_threshold * 100}%\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    training_history = {\n",
        "        'stages': [],\n",
        "        'iterations': [],\n",
        "        'success_rates': [],\n",
        "        'avg_rewards': [],\n",
        "        'policy_losses': [],\n",
        "        'value_losses': []\n",
        "    }\n",
        "    \n",
        "    total_iterations = 0\n",
        "    \n",
        "    # Progress through curriculum stages\n",
        "    for stage_idx, start_index in enumerate(curriculum_stages):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"STAGE {stage_idx + 1}/{len(curriculum_stages)}: Starting from index {start_index} ({start_index}/{traj_length})\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        stage_converged = False\n",
        "        stage_iteration = 0\n",
        "        \n",
        "        while not stage_converged and stage_iteration < max_iterations_per_phase:\n",
        "            total_iterations += 1\n",
        "            stage_iteration += 1\n",
        "            \n",
        "            # Collect trajectories\n",
        "            batch_data = collect_trajectories(\n",
        "                env, policy, episodes_per_iteration, \n",
        "                start_index=start_index, \n",
        "                reference_trajectory=reference_trajectory\n",
        "            )\n",
        "            \n",
        "            # Compute advantages and returns\n",
        "            advantages, returns = compute_gae(\n",
        "                batch_data['rewards'], \n",
        "                batch_data['values'], \n",
        "                batch_data['dones'], \n",
        "                gamma=gamma\n",
        "            )\n",
        "            \n",
        "            # PPO update\n",
        "            losses = ppo_update(\n",
        "                policy, optimizer, batch_data, advantages, returns,\n",
        "                clip_range=0.2, value_coef=0.5, entropy_coef=0.01, epochs=4\n",
        "            )\n",
        "            \n",
        "            # Evaluate policy\n",
        "            success_rate, avg_reward = evaluate_policy(\n",
        "                env, policy, num_episodes=20,\n",
        "                start_index=start_index,\n",
        "                reference_trajectory=reference_trajectory,\n",
        "                deterministic=True\n",
        "            )\n",
        "            \n",
        "            # Record metrics\n",
        "            training_history['stages'].append(stage_idx)\n",
        "            training_history['iterations'].append(total_iterations)\n",
        "            training_history['success_rates'].append(success_rate)\n",
        "            training_history['avg_rewards'].append(avg_reward)\n",
        "            training_history['policy_losses'].append(losses['policy_loss'])\n",
        "            training_history['value_losses'].append(losses['value_loss'])\n",
        "            \n",
        "            # Progress reporting\n",
        "            if stage_iteration % 5 == 0 or success_rate >= success_threshold:\n",
        "                print(f\"  Iter {stage_iteration:3d} | Success: {success_rate*100:5.1f}% | \"\n",
        "                      f\"Avg Reward: {avg_reward:.3f} | \"\n",
        "                      f\"Policy Loss: {losses['policy_loss']:7.4f} | \"\n",
        "                      f\"Value Loss: {losses['value_loss']:7.4f}\")\n",
        "            \n",
        "            # Check if we've converged on this stage\n",
        "            if success_rate >= success_threshold:\n",
        "                stage_converged = True\n",
        "                print(f\"\\n  âœ“ Stage {stage_idx + 1} converged! Success rate: {success_rate*100:.1f}%\")\n",
        "        \n",
        "        if not stage_converged:\n",
        "            print(f\"\\n  âš  Stage {stage_idx + 1} did not fully converge (max iterations reached)\")\n",
        "            print(f\"  Final success rate: {success_rate*100:.1f}%\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BACKWARD ALGORITHM TRAINING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return training_history\n",
        "\n",
        "print(\"Backward Algorithm function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train the Policy with Backward Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the policy using Backward Algorithm + PPO\n",
        "training_history = backward_algorithm_ppo(\n",
        "    env=env,\n",
        "    policy=policy_network,\n",
        "    reference_trajectory=best_trajectory,\n",
        "    learning_rate=3e-4,\n",
        "    gamma=0.99,\n",
        "    success_threshold=0.9,\n",
        "    max_iterations_per_phase=100,\n",
        "    episodes_per_iteration=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Training Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training progress\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Define colors for each curriculum stage\n",
        "stage_colors = plt.cm.tab10(np.linspace(0, 1, 5))\n",
        "\n",
        "# Plot 1: Success Rate over Training\n",
        "ax = axes[0, 0]\n",
        "for stage_idx in range(5):\n",
        "    mask = np.array(training_history['stages']) == stage_idx\n",
        "    if np.any(mask):\n",
        "        iterations = np.array(training_history['iterations'])[mask]\n",
        "        success_rates = np.array(training_history['success_rates'])[mask]\n",
        "        ax.plot(iterations, success_rates, 'o-', color=stage_colors[stage_idx], \n",
        "                label=f'Stage {stage_idx + 1}', alpha=0.7, linewidth=2)\n",
        "\n",
        "ax.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='Success Threshold (90%)')\n",
        "ax.set_xlabel('Training Iteration', fontsize=12)\n",
        "ax.set_ylabel('Success Rate', fontsize=12)\n",
        "ax.set_title('Success Rate Across Curriculum Stages', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim([0, 1.05])\n",
        "\n",
        "# Plot 2: Average Reward over Training\n",
        "ax = axes[0, 1]\n",
        "for stage_idx in range(5):\n",
        "    mask = np.array(training_history['stages']) == stage_idx\n",
        "    if np.any(mask):\n",
        "        iterations = np.array(training_history['iterations'])[mask]\n",
        "        avg_rewards = np.array(training_history['avg_rewards'])[mask]\n",
        "        ax.plot(iterations, avg_rewards, 'o-', color=stage_colors[stage_idx], \n",
        "                label=f'Stage {stage_idx + 1}', alpha=0.7, linewidth=2)\n",
        "\n",
        "ax.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Goal Reward (1.0)')\n",
        "ax.set_xlabel('Training Iteration', fontsize=12)\n",
        "ax.set_ylabel('Average Reward', fontsize=12)\n",
        "ax.set_title('Average Reward Across Curriculum Stages', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Policy Loss\n",
        "ax = axes[1, 0]\n",
        "for stage_idx in range(5):\n",
        "    mask = np.array(training_history['stages']) == stage_idx\n",
        "    if np.any(mask):\n",
        "        iterations = np.array(training_history['iterations'])[mask]\n",
        "        policy_losses = np.array(training_history['policy_losses'])[mask]\n",
        "        ax.plot(iterations, policy_losses, 'o-', color=stage_colors[stage_idx], \n",
        "                label=f'Stage {stage_idx + 1}', alpha=0.7, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Training Iteration', fontsize=12)\n",
        "ax.set_ylabel('Policy Loss', fontsize=12)\n",
        "ax.set_title('PPO Policy Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Value Loss\n",
        "ax = axes[1, 1]\n",
        "for stage_idx in range(5):\n",
        "    mask = np.array(training_history['stages']) == stage_idx\n",
        "    if np.any(mask):\n",
        "        iterations = np.array(training_history['iterations'])[mask]\n",
        "        value_losses = np.array(training_history['value_losses'])[mask]\n",
        "        ax.plot(iterations, value_losses, 'o-', color=stage_colors[stage_idx], \n",
        "                label=f'Stage {stage_idx + 1}', alpha=0.7, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Training Iteration', fontsize=12)\n",
        "ax.set_ylabel('Value Loss', fontsize=12)\n",
        "ax.set_title('PPO Value Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"- Each colored line represents a curriculum stage (starting point in trajectory)\")\n",
        "print(\"- Stage 1 starts closest to the goal (easiest), Stage 5 starts from the beginning (hardest)\")\n",
        "print(\"- The Backward Algorithm progressively increases difficulty as the policy improves\")\n",
        "print(\"- Success rates should increase within each stage as training progresses\")\n",
        "print(\"- Moving to an earlier stage typically causes a temporary drop in success rate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Policy Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained policy on full episodes (start_index=0)\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL POLICY EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "final_success_rate, final_avg_reward = evaluate_policy(\n",
        "    env=env,\n",
        "    policy=policy_network,\n",
        "    num_episodes=100,\n",
        "    start_index=0,  # Full episodes from the beginning\n",
        "    reference_trajectory=None,\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "print(f\"\\nResults over 100 test episodes:\")\n",
        "print(f\"  Success Rate: {final_success_rate * 100:.1f}%\")\n",
        "print(f\"  Average Reward: {final_avg_reward:.3f}\")\n",
        "print(f\"  Successful Episodes: {int(final_success_rate * 100)}/100\")\n",
        "\n",
        "if final_success_rate >= 0.9:\n",
        "    print(\"\\nâœ“ Policy successfully robustified! Achieving >90% success rate.\")\n",
        "else:\n",
        "    print(f\"\\nâš  Policy achieved {final_success_rate*100:.1f}% success rate.\")\n",
        "    print(\"  Consider training for more iterations or adjusting hyperparameters.\")\n",
        "\n",
        "# Demonstrate a few sample trajectories\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE POLICY TRAJECTORIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(5):\n",
        "    state, _ = env.reset()\n",
        "    trajectory = []\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    max_steps = 100\n",
        "    \n",
        "    while not done and steps < max_steps:\n",
        "        action, _, _ = policy_network.get_action(state, deterministic=True)\n",
        "        trajectory.append(action)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "    \n",
        "    trajectory_str = ' â†’ '.join([action_names[a] for a in trajectory])\n",
        "    status = \"âœ“ SUCCESS\" if total_reward >= 1.0 else \"âœ— FAILED\"\n",
        "    print(f\"\\nEpisode {i+1}: {status}\")\n",
        "    print(f\"  Length: {len(trajectory)} steps\")\n",
        "    print(f\"  Reward: {total_reward:.1f}\")\n",
        "    print(f\"  Actions: {trajectory_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Phase 2 Summary and Conclusions\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "This Phase 2 implementation successfully demonstrates the **robustification** phase of Go-Explore:\n",
        "\n",
        "1. **Converted brittle trajectories into robust policies**: The Phase 1 exploration discovered a solution trajectory, but it only worked through exact replay. Phase 2 trained a neural network policy that can solve the task independently.\n",
        "\n",
        "2. **Implemented the Backward Algorithm**: Rather than training from scratch, we used a curriculum learning approach that:\n",
        "   - Started training from near the goal (high success rate, clear learning signal)\n",
        "   - Progressively moved the starting point earlier in the trajectory\n",
        "   - Advanced to the next stage only when achieving >90% success\n",
        "   - Built competence incrementally from easy to hard\n",
        "\n",
        "3. **Used PPO for policy optimization**: We implemented the core PPO algorithm with:\n",
        "   - Clipped objective (Îµ = 0.2) to prevent large policy updates\n",
        "   - Actor-Critic architecture with separate policy and value heads\n",
        "   - GAE (Generalized Advantage Estimation) for variance reduction\n",
        "   - Entropy bonus to encourage exploration\n",
        "\n",
        "### Connection to the Original Go-Explore Paper\n",
        "\n",
        "**What matches the paper:**\n",
        "- âœ“ Two-phase approach (exploration â†’ robustification)\n",
        "- âœ“ Backward Algorithm curriculum learning\n",
        "- âœ“ PPO as the policy optimization algorithm\n",
        "- âœ“ Converting archive trajectories into robust policies\n",
        "- âœ“ Progressive difficulty increase as the policy improves\n",
        "\n",
        "**What differs (due to environment simplicity):**\n",
        "- Original: CNN-based policy for Atari pixel inputs\n",
        "- Our version: MLP for discrete state inputs (one-hot encoded)\n",
        "- Original: Thousands of training episodes on complex environments\n",
        "- Our version: Fewer episodes on simpler FrozenLake environment\n",
        "\n",
        "**Key conceptual alignment:**\n",
        "> \"The second phase ('robustification') trains a policy via imitation learning to robustly navigate to the goal.\" â€” Ecoffet et al., 2019\n",
        "\n",
        "This is exactly what we implemented: using the Phase 1 solution as a training curriculum for a robust neural network policy.\n",
        "\n",
        "### Why the Backward Algorithm Works\n",
        "\n",
        "The Backward Algorithm solves a fundamental problem in sparse-reward RL:\n",
        "\n",
        "**Problem without curriculum:**\n",
        "- Training from the start state requires discovering the entire solution through trial and error\n",
        "- Reward signal is only received at the goal (very sparse)\n",
        "- Early in training, the policy never reaches the goal, so it has no learning signal\n",
        "- This leads to extremely slow or failed learning\n",
        "\n",
        "**Solution with Backward Algorithm:**\n",
        "- Start training near the goal where success is easy to achieve\n",
        "- The policy quickly learns: \"from these states, do X to reach the goal\"\n",
        "- Once mastered, move the starting point earlier (slightly harder)\n",
        "- The policy builds on previous knowledge, learning incrementally\n",
        "- Continue until the full task can be solved from the initial state\n",
        "\n",
        "This is analogous to teaching someone a complex skill by starting with the final step, then adding earlier steps once each is mastered.\n",
        "\n",
        "### Potential Extensions\n",
        "\n",
        "This implementation could be extended in several ways to more closely match the full Go-Explore paper:\n",
        "\n",
        "1. **Stochastic environments**: Test robustness by enabling `is_slippery=True` in FrozenLake\n",
        "2. **Domain randomization**: Train on variations of the environment to improve generalization\n",
        "3. **Backward algorithm refinements**: Use more granular curriculum stages\n",
        "4. **Alternative robustification**: Try behavioral cloning or other imitation learning approaches\n",
        "5. **More complex environments**: Apply to environments with visual observations (e.g., Atari)\n",
        "\n",
        "### Final Thoughts\n",
        "\n",
        "Phase 2 transforms the exploration success of Phase 1 into a deployable policy. While Phase 1's deterministic trajectory replay is fragile, the trained neural network policy is:\n",
        "- **Generalizable**: Works from any starting state\n",
        "- **Robust**: Can handle minor perturbations (and with training, stochasticity)\n",
        "- **Efficient**: Makes decisions in real-time without needing archive lookup\n",
        "- **Transferable**: Can potentially adapt to related tasks\n",
        "\n",
        "This two-phase approachâ€”systematic exploration followed by robust policy learningâ€”is what makes Go-Explore so effective on hard-exploration problems.\n",
        "\n",
        "### References\n",
        "\n",
        "- **Original Paper**: Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2019). *Go-Explore: A New Approach for Hard-Exploration Problems*. arXiv:1901.10995\n",
        "- **Paper Link**: https://huggingface.co/papers/1901.10995\n",
        "- **PPO Paper**: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). *Proximal Policy Optimization Algorithms*. arXiv:1707.06347\n",
        "\n",
        "---\n",
        "\n",
        "**End of Phase 2 Implementation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Phase 1 vs Phase 2 Comparison (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison: Phase 1 trajectory vs Phase 2 trained policy\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 1 vs PHASE 2 COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nPhase 1 â€” Exploration (Archive-based trajectory):\")\n",
        "print(f\"  Method: Deterministic trajectory replay from archive\")\n",
        "print(f\"  Best trajectory length: {len(best_trajectory)} steps\")\n",
        "print(f\"  Success guarantee: 100% (in deterministic environment)\")\n",
        "print(f\"  Limitations:\")\n",
        "print(f\"    - Requires storing entire trajectory\")\n",
        "print(f\"    - Only works with exact replay (no generalization)\")\n",
        "print(f\"    - Breaks with any environmental stochasticity\")\n",
        "print(f\"    - Cannot handle unexpected states\")\n",
        "\n",
        "trajectory_str = ' â†’ '.join([action_names[a] for a in best_trajectory])\n",
        "print(f\"\\n  Best trajectory: {trajectory_str}\")\n",
        "\n",
        "print(\"\\nPhase 2 â€” Robustification (Trained neural policy):\")\n",
        "print(f\"  Method: MLP policy trained with PPO + Backward Algorithm\")\n",
        "print(f\"  Network parameters: {sum(p.numel() for p in policy_network.parameters()):,}\")\n",
        "print(f\"  Success rate: {final_success_rate * 100:.1f}% (over 100 episodes)\")\n",
        "print(f\"  Advantages:\")\n",
        "print(f\"    - Generalizes to any starting state\")\n",
        "print(f\"    - Fast inference (no trajectory replay needed)\")\n",
        "print(f\"    - Can potentially handle stochasticity\")\n",
        "print(f\"    - Compact representation (neural network weights)\")\n",
        "\n",
        "# Show one sample trajectory from the trained policy\n",
        "state, _ = env.reset()\n",
        "policy_trajectory = []\n",
        "done = False\n",
        "steps = 0\n",
        "\n",
        "while not done and steps < 100:\n",
        "    action, _, _ = policy_network.get_action(state, deterministic=True)\n",
        "    policy_trajectory.append(action)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    steps += 1\n",
        "\n",
        "policy_traj_str = ' â†’ '.join([action_names[a] for a in policy_trajectory])\n",
        "print(f\"\\n  Sample policy trajectory: {policy_traj_str}\")\n",
        "print(f\"  Length: {len(policy_trajectory)} steps (cf. Phase 1: {len(best_trajectory)} steps)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY INSIGHT:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Phase 1 discovers 'what to do' through systematic exploration.\")\n",
        "print(\"Phase 2 learns 'how to do it robustly' through policy optimization.\")\n",
        "print(\"\\nTogether, they solve hard-exploration problems that defeat conventional RL.\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
